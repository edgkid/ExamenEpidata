<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>The benefits and limitations of flexible array members</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/29/benefits-limitations-flexible-array-members" /><author><name>Serge Guelton</name></author><id>31f50823-24d0-4da8-9a0f-8cfc13ab586d</id><updated>2022-09-29T07:00:00Z</updated><published>2022-09-29T07:00:00Z</published><summary type="html">&lt;p&gt;Flexible array members (FAM) is an extension of C89 standardized in C99. This article discusses how flexible array members offer convenience and improve performance and how compiler implementations can generate complications.&lt;/p&gt; &lt;p&gt;FAM makes it possible to declare a struct with a dynamic size while keeping a flat memory layout. This is a textbook example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double data[ ]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;data&lt;/code&gt; array starts empty and will be loaded later, perhaps many times. Presumably, the programmer uses the &lt;code&gt;size&lt;/code&gt; member to hold the current number of elements and updates that variable with each change to the &lt;code&gt;data &lt;/code&gt;size.&lt;/p&gt; &lt;h2&gt;Flexible array members vs. pointer implementation&lt;/h2&gt; &lt;p&gt;Flexible array members allow faster allocation, better locality, and solid code generation. The feature is an alternative to a more traditional declaration of &lt;code&gt;data&lt;/code&gt; as a pointer:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double *data; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the pointer implementation, adding an array element requires an extra load initializing the structure on the heap. Each element added to the array requires two allocations for the object and its data member. The process results in fragmented memory between the object and the area pointed at by &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Standard flexible array member behavior&lt;/h2&gt; &lt;p&gt;The C99 standard, section 6.7.2.1.16, defines flexible array members. A struct with a flexible array member behaves in interesting ways.&lt;/p&gt; &lt;p&gt;It is legal to access any index of &lt;code&gt;fam::data&lt;/code&gt;, providing enough memory has been allocated:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam * f = malloc(sizeof(struct fam) + sizeof(double[n])); f - &gt; size = n;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;sizeof&lt;/code&gt; operator behaves as if the FAM had zero elements but accounts for the padding required to position it correctly. For instance, &lt;code&gt;sizeof(struct {char c; float d[];}&lt;/code&gt; is unlikely to be equal to &lt;code&gt;sizeof(char)&lt;/code&gt; because of the padding required to correctly position &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The assignment operator does not copy the flexible array member, which probably explains why that operator is not part of the C++ standard.&lt;/p&gt; &lt;p&gt;This would be the end of this post if there were no nonconformant compiler extensions.&lt;/p&gt; &lt;h2&gt;Nonconforming compiler extensions&lt;/h2&gt; &lt;p&gt;Flexible array members are supported only by GCC and Clang in C89 and C++ as extensions. The extensions use alternate syntax, sometimes called a struct hack.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam_extension { int size; double data[0]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can specify:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam_extension { int size; double data[1]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As it turns out, this syntax extended to any array size due to prior art, as suggested in the FreeBSD developers handbook, &lt;a href="https://docs.freebsd.org/en/books/developers-handbook/sockets/#sockets-sockaddr"&gt;section 7.5.1.1.2 sockaddr&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct sockaddr { unsigned char sa_len; /* total length */ sa_family_t sa_family; /* address family */ char sa_data[14]; /* actually longer; address value */ };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that using an array size different from 0 for the FAM makes the allocation idiom more complex because one needs to subtract the size of the FAM:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam * f = malloc(sizeof(struct sockaddr) + sizeof(char[n]) - sizeof(char[14]));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GCC and Clang extensions normalize the undefined behavior when performing an out-of-bounds access on an array. The program performs regular memory access as if it allocated the memory.&lt;/p&gt; &lt;h2&gt;Limitations of sized arrays&lt;/h2&gt; &lt;p&gt;The ability to consider sized arrays as FAM impacts the accuracy of some kinds of code analysis. Consider, for instance, the &lt;code&gt;-fsanitize=bounds&lt;/code&gt; option in which the instruments array detects when they are out-of-bounds. Without any context information, it cannot add a check to the following access:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double data[]; }; int foo(struct fam* f) { return f -&gt; data[8]; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But if we declare the array as &lt;code&gt;double data[1]&lt;/code&gt;, there is still no instrumentation. The compiler detects a FAM based on the extension definition and performs no check. Even worse, if we declare the array as &lt;code&gt;double data[4]&lt;/code&gt;, trunk GCC performs no check (honoring legacy code, as illustrated in the previous section), while Clang adds a bounds check.&lt;/p&gt; &lt;p&gt;We observe the same behavior for the &lt;code&gt;__builtin_object_size&lt;/code&gt; builtin. This builtin computes the allocated memory reachable from a pointer. When asked for &lt;code&gt;__builtin_object_size(f - &gt; data, 1)&lt;/code&gt;, both GCC and Clang return &lt;code&gt;-1&lt;/code&gt; (indicating a failure to compute that size) for all the declarations of &lt;code&gt;data&lt;/code&gt; we have explored so far. This policy is conservative and removes some of the security offered by &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt;, which relies heavily on the accuracy of &lt;code&gt;__builtin_object_size&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Motivation for stricter standard conformance&lt;/h2&gt; &lt;p&gt;A codebase that strictly conforms to the C99 standard (at least for FAM) would benefit from a compiler strictly following the standard definition of flexible array members. That goal motivates an effort currently led within the Linux kernel community, as demonstrated by &lt;a href="https://lore.kernel.org/lkml/20220322184802.GA2533969@embeddedor/"&gt;this patch&lt;/a&gt;. The &lt;a href="https://www.kernel.org/doc/html/v5.16/process/deprecated.html#zero-length-and-one-element-arrays"&gt;documentation&lt;/a&gt; update favors C99 FAM in place of zero-length arrays.&lt;/p&gt; &lt;p&gt;To take advantage of this development, they developed a compiler option using GCC and Clang to give the programmer control over flexible array syntax. The option is &lt;code&gt;-fstrict-flex-arrays=&lt;/code&gt; whereas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0 reflects the current situation described earlier.&lt;/li&gt; &lt;li&gt;1 considers only &lt;code&gt;[0]&lt;/code&gt;, &lt;code&gt;[1]&lt;/code&gt; and &lt;code&gt;[ ]&lt;/code&gt; as a FAM.&lt;/li&gt; &lt;li&gt;2 considers only &lt;code&gt;[0] &lt;/code&gt;and &lt;code&gt;[ ]&lt;/code&gt;as a FAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Compiling code with a &lt;code&gt;-fstrict-flex-arrays&lt;/code&gt; value greater than 0 unlocks some extra security while breaking (some) backward compatibility, which is why n=0 remains the default.&lt;/p&gt; &lt;h2&gt;Compiler convergence on C-language flexible array members&lt;/h2&gt; &lt;p&gt;Flexible array members is an interesting C99 feature that found its way, through compiler extensions, into C89 and C++. These extensions and legacy codes led to suboptimal code checks in the compiler, which the &lt;code&gt;-fstrict-flex-arrays=&lt;/code&gt; option can now control.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/29/benefits-limitations-flexible-array-members" title="The benefits and limitations of flexible array members"&gt;The benefits and limitations of flexible array members&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serge Guelton</dc:creator><dc:date>2022-09-29T07:00:00Z</dc:date></entry><entry><title type="html">WildFly 27 Beta1 is released!</title><link rel="alternate" href="https://wildfly.org//news/2022/09/29/WildFly26-Beta-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2022/09/29/WildFly26-Beta-Released/</id><updated>2022-09-29T00:00:00Z</updated><content type="html">I’m pleased to announce that the new WildFly and WildFly Preview 27.0.0.Beta1 releases are available for download at . NEW AND NOTABLE As I noted in , the key new thing in WildFly 27 is compatibility with the . Thanks so much to all the great WildFly contributors who worked so hard on our EE 10 journey! Besides Jakarta EE 10 support, here are some of the other new features in this release. Observability * - A new tech preview 'Micrometer' subsystem (WildFly Preview only) EJB * - A new 'distributable-ejb' subsystem *  — Support for use of an Infinispan cache as an EJB timer store Security * - Encryption support in the FileSystemSecurityRealm * - Identity integrity support in the FileSystemSecurityRealm Provisioning * - A new 'embedded-activemq' Galleon layer for embedded broker messaging * - A new 'hibernate-search' Galleon layer *  — New 'singleton-local' and 'singleton-ha' Galleon layers Major Component Upgrades vs WildFly 26.x There have been numerous component upgrades related to WildFly’s transition from the javax namespace EE 8 to the jakarta namespace EE 10; too many to list here. However, besides those EE 10 driven updates, there are a number of other major component updates in WildFly 27 Beta1: * Hibernate 6.1 replaces Hibernate 5.3 * Hibernate Search 6.1 replaces Hibernate Search 5.10 * Infinispan 14 replaces Infinispan 13 * JGroups 5.2 replaces JGroups 4.2 * RESTEasy 6.2 replaces RESTEasy 4.7 * Weld 5 replaces Weld 3.1 REMOVAL OF SUPPORT FOR EE 8 AND SE 8 As I discussed in my post, Jakarta EE 8 and Jakarta SE 8 are no longer supported beginning with the WildFly 27 series. Use of SE 11 or later is required, with SE 17 recommended. Testing on SE 19 is in progress. RELEASE NOTES The release notes for the release are . Note that this only covers the differences between 27 Beta1 and the recent 27 Alpha5; release notes for the 27 Alpha1 - 5 releases are available . Issues fixed in the numerous underlying WildFly Core 19.0 beta releases are listed . Please try it out and give us your feedback, while we get to work on WildFly 27 Final! Best regards, Brian</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title>Build a Kogito Serverless Workflow using Serverless Framework</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" /><author><name>Daniele Martinoli</name></author><id>59a057cf-6cdf-418b-8bb1-b494473d43b7</id><updated>2022-09-28T07:00:00Z</updated><published>2022-09-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="http://serverlessworkflow.io/"&gt;Serverless Workflow&lt;/a&gt; is a standard from the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF). &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-orchestrating-serverless"&gt;Kogito&lt;/a&gt; implements the Serverless Workflow specifications to define workflows for event-driven, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications using a DSL-based model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.serverless.com/"&gt;Serverless Framework&lt;/a&gt; is an open source framework that builds, compiles, and packages code for serverless deployment. The framework provides implementations for different cloud providers, including &lt;a href="https://knative.dev"&gt;Knative&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article walks you through the steps to integrate Kogito with Serverless Framework to build a working example on the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; Platform. The article is based on code you can find in &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework"&gt;my GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To run the demo in this article, you need the following tools on your local system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maven (at least 3.8.6)&lt;/li&gt; &lt;li&gt;Java SDK 11+&lt;/li&gt; &lt;li&gt;Docker&lt;/li&gt; &lt;li&gt;Bash terminal&lt;/li&gt; &lt;li&gt;The &lt;a href="https://www.serverless.com/framework/docs/getting-started"&gt;serverless command-line interface&lt;/a&gt; (CLI) from the Serverless Framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You also need accounts on the following systems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenShift 4.8+ (logged in as an account with the &lt;code&gt;cluster-admin&lt;/code&gt; role)&lt;/li&gt; &lt;li&gt;A &lt;a href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt; credential&lt;/li&gt; &lt;li&gt;A &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt; account&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Introducing the Kogito Newsletter Subscription Showcase example&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription"&gt;Kogito Newsletter Subscription Showcase&lt;/a&gt; is a demo based on the Kogito implementation of the Serverless Workflow specification. This example consists of two applications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;subscription-flow&lt;/code&gt;: The workflow orchestrator, defined using the Serverless Workflow specification&lt;/li&gt; &lt;li&gt;&lt;code&gt;subscription-service&lt;/code&gt;: A &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; application implementing the orchestrated services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 1 shows the system architecture of the example application.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/architecture_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/architecture_2.png?itok=D3tkbG8u" width="695" height="251" alt="The Newsletter Subscription Showcase is made of two applications, the Subscription Flow and the Subscription Service, an event Broker and a Persistent Storage " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt; KIE group &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-credit-line field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Credit Line&lt;/span&gt; &lt;span class="rhd-media-credit field__item"&gt; Thanks to KIE https://www.kie.org/ &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription &lt;/span&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The architecture of the Newsletter Subscription Showcase example. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The documentation in the repository describes how to &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription#running-on-knative"&gt;deploy and run the application on Knative&lt;/a&gt;, using the YAML configurations generated by the Maven build of the project, through the &lt;code&gt;knative&lt;/code&gt; build profile, running on &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;minikube&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article shows how to implement the same deployment in the cloud using the Serverless Framework. Our target is a Knative environment installed on OpenShift, but the principles extend to other cloud settings as well.&lt;/p&gt; &lt;h2 id="buildingkogito"&gt;Images for the Kogito Newsletter Subscription Showcase&lt;/h2&gt; &lt;p&gt;The first step is to build and publish the images of the two applications that make up the Newsletter Subscription Showcase example: &lt;code&gt;subscription-flow&lt;/code&gt; and &lt;code&gt;subscription-service&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You can download prebuilt images or build new ones from source. The prebuilt images for our example are available in the &lt;a data-saferedirecturl="https://www.google.com/url?q=http://quay.io/dmartino&amp;amp;source=gmail&amp;amp;ust=1664330157515000&amp;amp;usg=AOvVaw2UuZVMmvMJ1VTnzF74pQqa" href="http://quay.io/dmartino" target="_blank"&gt;quay.io/dmartino&lt;/a&gt; repository; if you download them, you can skip ahead to the next section, entitled "Installing Knative on OpenShift."&lt;/p&gt; &lt;p&gt;If you prefer to build the images yourself, you'll need to clone the Kogito Examples repository, build the applications using the &lt;code&gt;knative&lt;/code&gt; profile, and finally push the Docker images to your Quay repository, using the following commands. Replace &lt;code&gt;QUAY_USER_ID&lt;/code&gt; with your actual ID.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/kiegroup/kogito-examples.git cd kogito-examples git checkout stable cd serverless-workflow-examples/serverless-workflow-newsletter-subscription docker login quay.io mvn clean install -DskipTests -Pknative \ -Dquarkus.container-image.registry=quay.io \ -Dquarkus.container-image.group=QUAY_USER_ID \ -Dquarkus.container-image.push=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify that two images have been generated with the expected tag (as of today, it is &lt;code&gt;1.25.0.Final&lt;/code&gt;) on your &lt;a href="https://quay.io/repository/"&gt;Quay.io&lt;/a&gt; account, and modify the visibility of the images to make them publicly accessible.&lt;/p&gt; &lt;h2&gt;Installing Knative on OpenShift&lt;/h2&gt; &lt;p&gt;Knative can be easily installed on OpenShift using the OpenShift Serverless Operator, and this is the recommended approach we are going to follow.&lt;/p&gt; &lt;p&gt;Install the Red Hat Serverless Operator from the administrator console (Figure 2). The sequence of menu items to choose is &lt;strong&gt;OperatorHub→Red Hat OpenShift Serverless→Install.&lt;/strong&gt; Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/RHServerless.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/RHServerless.png?itok=XcrgtpH3" width="1440" height="747" alt="Administrator console -&gt; OperatorHub -&gt; Red Hat OpenShift Serverless -&gt; Install using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Select the Red Hat OpenShift Serverless operator from the OperatorHub page and install it using the default settings. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;One instance of the &lt;code&gt;KnativeServing&lt;/code&gt; custom resource is required to manage Knative serverless applications (Figure 3). Create the instance in the &lt;code&gt;knative-serving&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-serving&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Serving→Create KnativeServing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeServing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeServing1.png?itok=ASMZ9s82" width="1440" height="430" alt="Administrator console -&gt; Project: knative-serving -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Serving -&gt; Create KnativeServing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Install the KnativeServing custom resource from the knative-serving project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Additionally, one instance of the &lt;code&gt;KnativeEventing&lt;/code&gt; Custom Resource is required to manage the events around the Knative serverless applications (Figure 4). Create the instance in the &lt;code&gt;knative-eventing&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-eventing&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Eventing→Create KnativeEventing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeEventing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeEventing1.png?itok=01Fpe29z" width="1440" height="403" alt="Administrator console -&gt; Project: knative-eventing -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Eventing -&gt; Create KnativeEventing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Install the KnativeEventing custom resource from the knative-eventing project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Installing the example application&lt;/h2&gt; &lt;p&gt;To run the application, you need to install a PostgreSQL database. The application also requires some configuration changes to bring it up to date.&lt;/p&gt; &lt;h3&gt;Installing the newsletter-postgres service&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;newsletter-postgres&lt;/code&gt; service is a regular OpenShift deployment of PostgreSQL in a namespace called &lt;code&gt;newsletter-subscription-db&lt;/code&gt;. Execute the following instructions to install the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework.git cd kogito-serverless-workflow-with-serverless-framework oc create namespace newsletter-subscription-db oc adm policy add-scc-to-user anyuid -z default -n newsletter-subscription-db oc apply -f newsletter-postgres/newsletter-postgres.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Preparing the Serverless Framework&lt;/h3&gt; &lt;p&gt;To successfully run the example on OpenShift, we had to apply a few changes to the original implementation of the Knative Cloud Provider in the Serverless Framework. Such updates are needed to align the original Knative version to the one installed with the OpenShift Serverless Operator, and to introduce some extensions that support new settings and fix a few issues. Details are available in a &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework#updates-to-the-knative-provider"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because the example would not run using the default implementation of the Knative Cloud Provider, the &lt;code&gt;package.json&lt;/code&gt; descriptor includes the following dependency:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; "devDependencies": { "serverless-knative": "https://github.com/dmartinol/serverless-knative.git" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The necessary changes are available in the following GitHub repositories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/serverless-knative"&gt;Serverless Knative Plugin&lt;/a&gt;: The Knative Cloud Provider implementation for the Serverless Framework&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-serving"&gt;knative-serving&lt;/a&gt;: A Node.js module to manage Knative Serving instances&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-eventing"&gt;knative-eventing&lt;/a&gt;: A Node.js module to manage Knative Eventing instances&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Unwrapping the Serverless Framework descriptor&lt;/h3&gt; &lt;p&gt;The heart of the Serverless Framework deployment is the &lt;code&gt;serverless.yml&lt;/code&gt; file that sits at the local root of the &lt;code&gt;kogito-serverless-workflow-with-serverless-framework&lt;/code&gt; repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;service: newsletter frameworkVersion: '3' provider: name: knative # optional Docker Hub credentials you need if you're using local Dockerfiles as function handlers docker: username: ${env:DOCKER_HUB_USERNAME} password: ${env:DOCKER_HUB_PASSWORD} functions: event-display: handler: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display@sha256:a214514d6ba674d7393ec8448dd272472b2956207acb3f83152d3071f0ab1911 # autoscaler field is managed by knative provider # Just add any autoscaling related annotation and it will be propagated to the deployed Service and Revision # The plugin automatically adds the 'autoscaling.knative.dev/' prefix to the annotation name autoscaler: min-scale: 1 max-scale: 2 events: - custom: name: new.subscription.2.event-display filter: attributes: type: new.subscription - custom: name: confirm.subscription.2.event-display filter: attributes: type: confirm.subscription subscription-service: handler: Dockerfile.jvm context: ./subscription-service subscription-flow: handler: Dockerfile.jvm context: ./subscription-flow events: - custom: filter: attributes: type: confirm.subscription - sinkBinding: {} plugins: - serverless-knative&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To replicate the architecture of the original example, this deployment includes the functions listed in the following sections (the equivalents of the Knative Service resources).&lt;/p&gt; &lt;h4&gt;event-display&lt;/h4&gt; &lt;p&gt;This is an event logger application implemented with a prebuilt image from the Google Cloud Container Registry. The image is configured with a minimum of one instance to simplify logging activity, and has two &lt;code&gt;custom&lt;/code&gt; events that are mapped onto two Knative Trigger instances.&lt;/p&gt; &lt;h4&gt;subscription-service&lt;/h4&gt; &lt;p&gt;This is the service running the original &lt;code&gt;subscription-service&lt;/code&gt; application. The original source code was copied from the Kogito Examples repository under the &lt;code&gt;subscription-service&lt;/code&gt; folder, to show the option to locally build an application and deploy the serverless service using the Serverless Framework CLI.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;Dockerfile.jvm&lt;/code&gt; file defined by the &lt;code&gt;handler&lt;/code&gt; property builds the Quarkus application and injects the binding properties to connect to the &lt;code&gt;newsletter-postgres&lt;/code&gt; database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;ENV POSTGRES_PASSWORD=cGFzcwo= ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;subscription-flow&lt;/h4&gt; &lt;p&gt;This function runs the &lt;code&gt;subscription-flow&lt;/code&gt; image that you previously built, but with overridden properties to locate the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM quay.io/dmartino/serverless-workflow-newsletter-subscription-flow:1.25.0.Final ENV SUBSCRIPTION_API_URL=http://newsletter-subscription-service.sls-newsletter-dev.svc.cluster.local ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This serverless function is configured with one &lt;code&gt;custom&lt;/code&gt; event, mapped to a Knative Trigger instance, and one &lt;code&gt;sinkBinding&lt;/code&gt; event that generates the Knative SinkBinding to connect the Knative Service with the &lt;code&gt;default&lt;/code&gt; Knative Broker. The &lt;code&gt;default&lt;/code&gt; Knative Broker is automatically created by the &lt;code&gt;eventing.knative.dev/injection&lt;/code&gt; annotation attached to the Knative Trigger instances.&lt;/p&gt; &lt;h2&gt;Deploying the application with the Serverless Framework&lt;/h2&gt; &lt;p&gt;The first step is to build the &lt;code&gt;subscription-service&lt;/code&gt; application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd subscription-service $ mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following instructions assume that you have already installed the &lt;code&gt;serverless&lt;/code&gt; CLI, and set the environment variables &lt;code&gt;DOCKER_HUB_USERNAME&lt;/code&gt; and &lt;code&gt;DOCKER_HUB_PASSWORD&lt;/code&gt; to define the access credentials to the Docker Hub repository. Now deployment is just a matter of running the &lt;code&gt;serverless deploy&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless deploy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info&lt;/code&gt; command returns the deployment status of your application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless info ... Service Information service: newsletter namespace: sls-newsletter-dev Deployed functions event-display: - url: https://newsletter-event-display-sls-newsletter-dev.DOMAIN - custom - custom subscription-service: - url: https://newsletter-subscription-service-sls-newsletter-dev.DOMAIN subscription-flow: - url: https://newsletter-subscription-flow-sls-newsletter-dev.DOMAIN - custom - sinkBinding &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Validating the applications&lt;/h3&gt; &lt;p&gt;Run the applications using the default browser by executing the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ open -t $(oc get ksvc newsletter-subscription-flow -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}') $ open -t $(oc get ksvc newsletter-subscription-service -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}')&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can monitor the &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt; events through the logs of the &lt;code&gt;event-display&lt;/code&gt; pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -l serving.knative.dev/service=newsletter-event-display -f -n sls-newsletter-dev -c user-container&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Serverless Framework supports cloud deployments&lt;/h2&gt; &lt;p&gt;By using the Serverless Framework software, we successfully deployed a serverless application on Red Hat OpenShift, using Knative as the serverless framework.&lt;/p&gt; &lt;p&gt;The default implementation of the Knative Cloud Provider is missing some features and is not compatible with the Red Hat OpenShift Serverless Operator, so a patched implementation was used for the purposes of this article.&lt;/p&gt; &lt;p&gt;The application is defined using the Kogito implementation of the CNCF Serverless Workflow specification, a DSL-based model that targets the serverless technology domain.&lt;/p&gt; &lt;p&gt;Serverless Framework claims to be a cloud-agnostic tool, so nothing prevents us from extending the exercise in the future and adapting this deployment model to run on another cloud platform such as AWS or Azure.&lt;/p&gt; &lt;p&gt;For more information, please read the blog posting &lt;a href="https://knative.dev/blog/articles/event-drive-app-knative-eventing-kogito/"&gt;Orchestrating Events with Knative and Kogito&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" title="Build a Kogito Serverless Workflow using Serverless Framework"&gt;Build a Kogito Serverless Workflow using Serverless Framework&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniele Martinoli</dc:creator><dc:date>2022-09-28T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.13.0.Final released - Cross site request forgery prevention filter, Kafka Dev UI</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-13-0-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-13-0-final-released/</id><updated>2022-09-28T00:00:00Z</updated><content type="html">We don’t have a ton of big new features in Quarkus 2.13.0.Final but it comes with a ton of small enhancements that should improve your overall experience with Quarkus. It still comes with some exciting stuff: Cross Site Request Forgery (CSRF) prevention filter for RESTEasy Reactive (well, security is not...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">Jakarta Persistence 3.1 new features</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-persistence-3-1-new-features/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-persistence-3-1-new-features/</id><updated>2022-09-27T10:12:40Z</updated><content type="html">This tutorial introduces Jakarta Persistence API 3.1 as a standard for management of persistence and O/R mapping in Java environments. We will discuss the headlines with a simple example that you can test on a Jakarta EE 10 runtime. New features added in Jakarta Persistence 3.1 There are several new features available in Jakarta Persistence ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>End-to-end field-level encryption for Apache Kafka Connect</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/27/end-end-field-level-encryption-apache-kafka-connect" /><author><name>Hans-Peter Grahsl</name></author><id>ab96f1c9-28ec-49c7-b59d-50bfb106f33c</id><updated>2022-09-27T07:00:00Z</updated><published>2022-09-27T07:00:00Z</published><summary type="html">&lt;p&gt;Encryption is valuable in &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt;, as with other communication tools, for protecting data that might be sent to unanticipated or untrustworthy recipients. This series of articles introduces the open source &lt;a href="https://github.com/hpgrahsl/kryptonite-for-kafka"&gt;Kryptonite for Kafka&lt;/a&gt; library, which is a community project I wrote. Kryptonite for Kafka requires no changes to source code, as it works entirely through configuration files. It currently does so by encrypting data through integration with &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Apache Kafka Connect&lt;/a&gt;, but there are plans to extend the scope of the project to other integration strategies for Kafka.&lt;/p&gt; &lt;p&gt;This first article in the series demonstrates encryption on individual fields in structured data, using a relational database and a NoSQL database as examples. The second article focuses on files and introduces some additional sophistication, such as using different keys for different fields.&lt;/p&gt; &lt;h2&gt;How Kryptonite for Kafka reduces the risk of a data breach&lt;/h2&gt; &lt;p&gt;Kafka can take advantage of &lt;a href="https://kafka.apache.org/documentation/#security_overview"&gt;several security features&lt;/a&gt;, ranging from &lt;a href="https://kafka.apache.org/documentation/#security_sasl"&gt;authentication&lt;/a&gt; and &lt;a href="https://kafka.apache.org/documentation/#security_authz"&gt;authorization&lt;/a&gt; to TLS-based, over-the-wire &lt;a href="https://kafka.apache.org/documentation/#security_ssl"&gt;traffic encryption&lt;/a&gt; of data on its way in and out of Kafka topics. Although these measures secure data in transit, they take place within Kafka, so there is always a stage where the broker has to see plaintext data and temporarily keep it in memory.&lt;/p&gt; &lt;p&gt;This stage can be considered a blind spot in Kafka security. The data might be encrypted on disk, but the Kafka brokers see the plaintext data right before storing it, and decrypt the data temporarily every time they read it back from disk. Therefore, disk encryption alone cannot protect against RAM scraping and other clever attack vectors.&lt;/p&gt; &lt;p&gt;Kryptonite for Kafka plugs this in-memory loophole and offers added sophistication, such as the ability to encrypt specific fields in structured data.&lt;/p&gt; &lt;h2&gt;Encryption outside the Kafka brokers&lt;/h2&gt; &lt;p&gt;Let's imagine a generic Kafka Connect data integration. Assume we would like to encrypt a certain subset of sensitive fields found in a &lt;code&gt;ConnectRecord&lt;/code&gt; payload originating from any data source (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/01a_concept_field_level_enc_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/01a_concept_field_level_enc_1.png?itok=siy2xFy1" width="1440" height="605" alt="In the source record, the social security number needs to be protected through encryption." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: In the source record, the social security number needs to be protected through encryption. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;On the consumer side, we need to decrypt the same subset of previously encrypted fields in a &lt;code&gt;ConnectRecord&lt;/code&gt; payload directed to the data sink (Figure 2).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/01b_concept_field_level_dec_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/01b_concept_field_level_dec_0.png?itok=hp-Mmori" width="1440" height="604" alt="Before delivering the record to the data sink, the social security number is automatically decrypted." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Before delivering the record to the data sink, the social security number is automatically decrypted. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The social security number must be decrypted before it's delivered to the data sink.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;To make sure the Kafka brokers themselves never get to see—let alone directly store—the original plaintext for sensitive data fields, the encryption and decryption must happen outside of the brokers, a step represented by the pink question marks in Figures 1 and 2.&lt;/p&gt; &lt;h3&gt;Kryptonite for Kafka's approach to data encryption: An overview&lt;/h3&gt; &lt;p&gt;Before Kryptonite for Kafka, no flexible and convenient way to accomplish client-side field-level cryptography was available for Kafka in &lt;a href="https://developers.redhat.com/topics/open-source"&gt;free and open source software&lt;/a&gt;, neither with the standard features in Kafka Connect nor with additional libraries or free tools from Kafka Connect's open-source ecosystem.&lt;/p&gt; &lt;p&gt;Encryption could happen directly in the original client or in some intermediary process like a sidecar or a proxy, but an intermediary would likely impose higher deployment effort and an additional operational burden. To avoid this, Kryptonite for Kafka currently performs the encryption and decryption during a Kafka Connect integration. The worker nodes of a Kafka Connect cluster encrypt the fields designated as sensitive within &lt;code&gt;ConnectRecord&lt;/code&gt; instances.&lt;/p&gt; &lt;p&gt;For that purpose, the library provides a turnkey ready single message &lt;a href="https://kafka.apache.org/documentation/#connect_transforms"&gt;transform&lt;/a&gt; (SMT) to apply field-level encryption and decryption to Kafka Connect records. The system is agnostic to the type of message serialization chosen. It uses authenticated encryption with associated data (AEAD), and in particular applies &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/chap-encryption_standards#sec-AES"&gt;AES&lt;/a&gt; in either &lt;a href="https://www.ibm.com/docs/en/zos/2.3.0?topic=operation-galoiscounter-mode-gcm"&gt;GCM&lt;/a&gt; or &lt;a href="https://www.rfc-editor.org/rfc/rfc5297"&gt;SIV&lt;/a&gt; mode.&lt;/p&gt; &lt;p&gt;Each encrypted field is represented in the output as a Base64-encoded string that contains the ciphertext of the field's value along with metadata. The metadata consists of a version identifier for the Kryptonite for Kafka library itself, a short identifier for the encryption algorithm, and an identifier for the secret key material. The metadata is authenticated but not encrypted, so Kryptonite for Kafka on the consumer side can read it and use it to decrypt the data.&lt;/p&gt; &lt;p&gt;For schema-aware message formats such as AVRO, the original schema of a data record is redacted so that encrypted fields can be stored in Base64-encoded form, changing the original data types for the affected fields.&lt;/p&gt; &lt;p&gt;In a nutshell, the configurable &lt;code&gt;CipherField&lt;/code&gt; SMT can be plugged into arbitrary Kafka Connect pipelines, safeguarding sensitive and precious data against any form of uncontrolled or illegal access on the data's way into and out of Kafka brokers (Figures 3 and 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/03a_kryptonite_smt_source_enc.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/03a_kryptonite_smt_source_enc.png?itok=hIO6xIi5" width="1440" height="604" alt="Kafka Connect encrypts the data by piping it through the SMT before delivery to the brokers." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Kafka Connect encrypts the data by piping it through the SMT before delivery to the brokers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/03b_kryptonite_smt_sink_dec.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/03b_kryptonite_smt_sink_dec.png?itok=hBp39Gq5" width="1440" height="602" alt="Kafka Connect decrypts the data by piping it through the SMT before sink connectors deliver it to the target system." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Kafka Connect decrypts the data by piping it through the SMT before sink connectors deliver it to the target system. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Advantages of encryption outside the Kafka brokers&lt;/h3&gt; &lt;p&gt;With the Kryptonite for Kafka approach, sensitive fields are automatically and always secured, not only in transit but also at rest, whenever sensitive data is outside the Kafka Connect environment. Protection is guaranteed for all target systems and for whatever downstream consumers eventually get their hands on the data. Even if someone has access to the brokers, they cannot misuse the sensitive parts of the data unless they manage to steal the secret keys from (usually remote) client environments or break the underlying cryptography itself.&lt;/p&gt; &lt;p&gt;In short, Kryptonite for Kafka offers an additional layer of data security independent of whoever owns or operates the Kafka cluster, thus protecting the data against internal attackers. Encryption also protects data against external attackers who might get fraudulent access to the Kafka topic data in the future.&lt;/p&gt; &lt;p&gt;Given such a setup, you can precisely define which downstream consumers can read the sensitive data fields. In a Kafka Connect data integration pipeline, only sink connectors explicitly given access to the secret keys can successfully decrypt the protected data parts.&lt;/p&gt; &lt;h2 id="high-level-view"&gt;Example: Exchanging structured data between a relational database and a NoSQL database&lt;/h2&gt; &lt;p&gt;This example demonstrates field-level data protection during replication between two different types of databases. The source is &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; and the sink is &lt;a href="https://www.mongodb.com/"&gt;MongoDB&lt;/a&gt;. The exchange uses the open source &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt; platform for log-based change data capture (CDC). Kryptonite for Kafka post-processes Debezium's &lt;a href="https://debezium.io/documentation/reference/stable/transformations/event-flattening.html"&gt;MySQL CDC event payloads&lt;/a&gt; to encrypt certain fields before the data leaves Kafka Connect on the client side and reaches the Kafka brokers. The MongoDB sink connector that reads these Kafka records gets access to pre-processed, properly decrypted records, and at the end of the process stores plaintext documents into MongoDB collections (Figure 5).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/04_use_case_1_overview.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/04_use_case_1_overview.png?itok=f8JCYSFH" width="1440" height="812" alt="Kafka Connect encrypts data passed in from MySQL through Debezium and decrypts data just before passing it to MongoDB, so that the Kafka brokers see only encrypted data." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Kafka Connect encrypts data passed in from MySQL through Debezium and decrypts data just before passing it to MongoDB, so that the Kafka brokers see only encrypted data. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Producer-side encryption&lt;/h3&gt; &lt;p&gt;The MySQL instance stores, among other data, an &lt;code&gt;addresses&lt;/code&gt; table containing a couple of rows representing different kinds of fictional customer addresses (Figure 6).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/04_use_case_1_db_source_table.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/04_use_case_1_db_source_table.png?itok=S8TcICcU" width="1440" height="351" alt="A column for the customer's street is one of several columns in a MySQL table." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: A column for the customer's street is one of several columns in a MySQL table. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The following Debezium MySQL source connector captures all existing addresses, together with any future changes from the table:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"name": &lt;span class="hljs-string"&gt;"mysql-source-001", &lt;span class="hljs-attr"&gt;"config": { &lt;span class="hljs-attr"&gt;"connector.class": &lt;span class="hljs-string"&gt;"io.debezium.connector.mysql.MySqlConnector", &lt;span class="hljs-attr"&gt;"value.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"value.converter.schemas.enable": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"key.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"key.converter.schemas.enable": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"tasks.max": &lt;span class="hljs-string"&gt;"1", &lt;span class="hljs-attr"&gt;"database.hostname": &lt;span class="hljs-string"&gt;"mysql", &lt;span class="hljs-attr"&gt;"database.port": &lt;span class="hljs-string"&gt;"3306", &lt;span class="hljs-attr"&gt;"database.user": &lt;span class="hljs-string"&gt;"root", &lt;span class="hljs-attr"&gt;"database.password": &lt;span class="hljs-string"&gt;"debezium", &lt;span class="hljs-attr"&gt;"database.server.id": &lt;span class="hljs-string"&gt;"1234567", &lt;span class="hljs-attr"&gt;"database.server.name": &lt;span class="hljs-string"&gt;"mysqlhost", &lt;span class="hljs-attr"&gt;"database.whitelist": &lt;span class="hljs-string"&gt;"inventory", &lt;span class="hljs-attr"&gt;"table.whitelist": &lt;span class="hljs-string"&gt;"inventory.addresses", &lt;span class="hljs-attr"&gt;"database.history.kafka.bootstrap.servers": &lt;span class="hljs-string"&gt;"kafka:9092", &lt;span class="hljs-attr"&gt;"database.history.kafka.topic": &lt;span class="hljs-string"&gt;"mysqlhost-schema", &lt;span class="hljs-attr"&gt;"transforms": &lt;span class="hljs-string"&gt;"unwrap", &lt;span class="hljs-attr"&gt;"transforms.unwrap.type": &lt;span class="hljs-string"&gt;"io.debezium.transforms.ExtractNewRecordState", &lt;span class="hljs-attr"&gt;"transforms.unwrap.drop.tombstones": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"transforms.unwrap.delete.handling.mode": &lt;span class="hljs-string"&gt;"drop" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The preceding configuration is pretty straightforward, serializing CDC events as JSON without explicit schema information and unwrapping the Debezium payloads. Unsurprisingly, the resulting plaintext record values stored in the corresponding Kafka topic (called &lt;code&gt;mysqlhost.inventory.addresses&lt;/code&gt;) will look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"3787 Brownton Road", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's assume the task is now to make sure that values originating from the &lt;code&gt;addresses&lt;/code&gt; table's &lt;code&gt;street&lt;/code&gt; column (Figure 7) for all these CDC event payloads must not be stored in Kafka topics as plaintext.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Thanks to the field-level encryption capabilities of the custom &lt;code&gt;CipherField&lt;/code&gt; SMT, fine-grained protection can be easily achieved without writing any additional code. Simply add the following to the Debezium MySQL source connector configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-string"&gt;"name": &lt;span class="hljs-string"&gt;"mysql-source-enc-001", &lt;span class="hljs-string"&gt;"config": { &lt;span class="hljs-comment"&gt;/* ... */ &lt;span class="hljs-string"&gt;"transforms": &lt;span class="hljs-string"&gt;"unwrap,cipher", &lt;span class="hljs-comment"&gt;/* ... */ &lt;span class="hljs-string"&gt;"transforms.cipher.type": &lt;span class="hljs-string"&gt;"com.github.hpgrahsl.kafka.connect.transforms.kryptonite.CipherField$Value", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_mode": &lt;span class="hljs-string"&gt;"ENCRYPT", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_data_keys": &lt;span class="hljs-string"&gt;"&lt;span class="hljs-subst"&gt;${file:/secrets/classified.properties:cipher_data_keys}", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_data_key_identifier": &lt;span class="hljs-string"&gt;"my-demo-secret-key-123", &lt;span class="hljs-string"&gt;"transforms.cipher.field_config": &lt;span class="hljs-string"&gt;"[{\"name\&lt;span class="hljs-string"&gt;":\"street\&lt;span class="hljs-string"&gt;"}]", &lt;span class="hljs-string"&gt;"transforms.cipher.predicate":&lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-string"&gt;"transforms.cipher.negate":&lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-string"&gt;"predicates": &lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-string"&gt;"predicates.isTombstone.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.predicates.RecordIsTombstone" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The SMT configuration contains &lt;code&gt;transform.cipher.*&lt;/code&gt; properties with the following meanings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Operate on the records' values (&lt;code&gt;type: "CipherField$Value"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Encrypt data (&lt;code&gt;cipher_mode: "ENCRYPT"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Load the secret key material from an external key file (&lt;code&gt;cipher_data_keys: "${file:/secrets/classified.properties:cipher_data_keys}&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use a specific secret key based on its ID (&lt;code&gt;cipher_data_key_identifier: "my-demo-secret-key-123"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Process only the &lt;code&gt;street&lt;/code&gt; field (&lt;code&gt;field_config: "[{\"name\":\"street\"}]"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Ignore any tombstone records (see predicate definitions).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Maintaining the secrecy of the secret key materials is of utmost importance, because leaking any of the secret keys renders encryption useless. Secret key exchange is a complex topic of its own. This example obtains the keys indirectly from an external file, which contains a single property called &lt;code&gt;cipher_data_keys&lt;/code&gt; that in turn holds an array of key definition objects (&lt;code&gt;identifier&lt;/code&gt; and &lt;code&gt;material&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-properties"&gt;cipher_data_keys=[ { &lt;span class="hljs-string"&gt;"identifier": &lt;span class="hljs-string"&gt;"my-demo-secret-key-123", &lt;span class="hljs-string"&gt;"material": { &lt;span class="hljs-string"&gt;"primaryKeyId": &lt;span class="hljs-number"&gt;1000000001, &lt;span class="hljs-string"&gt;"key": [ { &lt;span class="hljs-string"&gt;"keyData": { &lt;span class="hljs-string"&gt;"typeUrl": &lt;span class="hljs-string"&gt;"type.googleapis.com/google.crypto.tink.AesGcmKey", &lt;span class="hljs-string"&gt;"value": &lt;span class="hljs-string"&gt;"GhDRulECKAC8/19NMXDjeCjK", &lt;span class="hljs-string"&gt;"keyMaterialType": &lt;span class="hljs-string"&gt;"SYMMETRIC" }, &lt;span class="hljs-string"&gt;"status": &lt;span class="hljs-string"&gt;"ENABLED", &lt;span class="hljs-string"&gt;"keyId": &lt;span class="hljs-number"&gt;1000000001, &lt;span class="hljs-string"&gt;"outputPrefixType": &lt;span class="hljs-string"&gt;"TINK" } ] } } ] &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details about externalizing sensitive configuration parameters in Kafka Connect can be found in the &lt;a href="https://github.com/hpgrahsl/kafka-connect-transform-kryptonite#externalize-configuration-parameters"&gt;library's documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The resulting source connector configuration achieves the goal of partially encrypting all Debezium CDC event payloads before they are sent to and stored by the Kafka brokers. The resulting record values in the topic &lt;code&gt;mysqlhost.inventory.addresses&lt;/code&gt; reflect the encryption:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"NLWw4AshpLIIBjLoqM0EgDiUGooYH3jwDnW71wdInMGomFVLHo9AQ6QPEh6fmLRJKVwE3gwwsWux", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Consumer-side decryption&lt;/h3&gt; &lt;p&gt;The sink connector that picks up these partially encrypted Kafka records needs to properly apply the custom &lt;code&gt;CipherField&lt;/code&gt; SMT as well. Only then can the connector get access to the previously encrypted Debezium CDC payload fields before writing them into the targeted data store, which is MongoDB in this case.&lt;/p&gt; &lt;p&gt;The MongoDB sink connector configuration for this example might look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"name": &lt;span class="hljs-string"&gt;"mongodb-sink-dec-001", &lt;span class="hljs-attr"&gt;"config": { &lt;span class="hljs-attr"&gt;"topics": &lt;span class="hljs-string"&gt;"mysqlhost.inventory.addresses", &lt;span class="hljs-attr"&gt;"connector.class": &lt;span class="hljs-string"&gt;"com.mongodb.kafka.connect.MongoSinkConnector", &lt;span class="hljs-attr"&gt;"key.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"key.converter.schemas.enable":&lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"value.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"value.converter.schemas.enable":&lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"tasks.max": &lt;span class="hljs-string"&gt;"1", &lt;span class="hljs-attr"&gt;"connection.uri":&lt;span class="hljs-string"&gt;"mongodb://mongodb:27017", &lt;span class="hljs-attr"&gt;"database":&lt;span class="hljs-string"&gt;"kryptonite", &lt;span class="hljs-attr"&gt;"document.id.strategy":&lt;span class="hljs-string"&gt;"com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy", &lt;span class="hljs-attr"&gt;"delete.on.null.values": &lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-attr"&gt;"transforms": &lt;span class="hljs-string"&gt;"createid,removefield,decipher", &lt;span class="hljs-attr"&gt;"transforms.createid.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.ReplaceField$Key", &lt;span class="hljs-attr"&gt;"transforms.createid.renames": &lt;span class="hljs-string"&gt;"id:_id", &lt;span class="hljs-attr"&gt;"transforms.removefield.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.ReplaceField$Value", &lt;span class="hljs-attr"&gt;"transforms.removefield.blacklist": &lt;span class="hljs-string"&gt;"id", &lt;span class="hljs-attr"&gt;"transforms.decipher.type": &lt;span class="hljs-string"&gt;"com.github.hpgrahsl.kafka.connect.transforms.kryptonite.CipherField$Value", &lt;span class="hljs-attr"&gt;"transforms.decipher.cipher_mode": &lt;span class="hljs-string"&gt;"DECRYPT", &lt;span class="hljs-attr"&gt;"transforms.decipher.cipher_data_keys": &lt;span class="hljs-string"&gt;"${file:/secrets/classified.properties:cipher_data_keys}", &lt;span class="hljs-attr"&gt;"transforms.decipher.field_config": &lt;span class="hljs-string"&gt;"[{\"name\":\"street\"}]", &lt;span class="hljs-attr"&gt;"transforms.decipher.predicate":&lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-attr"&gt;"transforms.decipher.negate":&lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-attr"&gt;"predicates": &lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-attr"&gt;"predicates.isTombstone.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.predicates.RecordIsTombstone" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The interesting part again is the configuration's &lt;code&gt;transform.decipher.*&lt;/code&gt; properties, which are defined as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Operate on the records' values (&lt;code&gt;type: "CipherField$Value"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Decrypt data (&lt;code&gt;cipher_mode: "DECRYPT"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Load the secret key material from an external key file (&lt;code&gt;cipher_data_keys: "${file:/secrets/classified.properties:cipher_data_keys}&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use a specific secret key based on its ID (&lt;code&gt;cipher_data_key_identifier: "my-secret-key-123"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Process only the &lt;code&gt;name&lt;/code&gt; field (&lt;code&gt;field_config: "[{\"name\":\"street\"}]"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Ignore any tombstone records (see predicate definitions).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this configuration in place, Kafka Connect processes all CDC events by first applying the custom &lt;code&gt;CipherField&lt;/code&gt; SMT to decrypt selected fields, and then handing them over to the sink connector itself to write the plaintext documents into a MongoDB database collection called &lt;code&gt;kryptonite.mysqlhost.inventory.addresses&lt;/code&gt;. An example document for the address having &lt;code&gt;_id=13&lt;/code&gt; is shown here in its JSON representation:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"_id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"3787 Brownton Road", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example has shown how end-to-end change data capture pipelines between heterogenous databases can be secured by explicitly protecting sensitive CDC payload fields. Proper configuration of a custom SMT is sufficient to achieve client-side field-level encryption and decryption of Kafka Connect records on their way in and out of Kafka topics. A &lt;a href="https://github.com/hpgrahsl/rhd-csflc-kafka-connect-demos/tree/main/use_case_1"&gt;fully working example&lt;/a&gt; of this database integration scenario can be found in the accompanying &lt;a href="https://github.com/hpgrahsl/rhd-csflc-kafka-connect-demos"&gt;demo scenario repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The second article in this series will show the use of Kryptonite for Kafka with files, introduce more features, and discuss plans for the future of the project.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/27/end-end-field-level-encryption-apache-kafka-connect" title="End-to-end field-level encryption for Apache Kafka Connect"&gt;End-to-end field-level encryption for Apache Kafka Connect&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Hans-Peter Grahsl</dc:creator><dc:date>2022-09-27T07:00:00Z</dc:date></entry><entry><title>Find errors in packages through mass builds</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/26/find-errors-packages-through-mass-builds" /><author><name>Frédéric Bérat</name></author><id>0ebf20fc-5f39-4cff-bf81-9b729880a1b3</id><updated>2022-09-26T07:00:00Z</updated><published>2022-09-26T07:00:00Z</published><summary type="html">&lt;p&gt;Even after thorough unit testing on both applications and their library dependencies, builds often go wrong and turn up hidden errors. This article introduces a new tool, the &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;Mass Prebuilder&lt;/a&gt; (MPB), that automates builds on enormous numbers of reverse dependencies to find problems that are not caught through package testing.&lt;/p&gt; &lt;p&gt;Let's look at a simple example. Roughly 1,200 packages in Red Hat-based distributions depend on &lt;a href="https://www.gnu.org/software/autoconf/"&gt;GNU Autoconf&lt;/a&gt;. Knowing all of the packages by heart is unlikely, building them manually would take ages, and judging whether a failure is due to a change in GNU Autoconf is very difficult. A job for the Mass Prebuilder!&lt;/p&gt; &lt;h2&gt;What is the Mass Prebuilder?&lt;/h2&gt; &lt;p&gt;The Mass Prebuilder is an open source set of tools that help developers create mass rebuilds around a limited set of packages, in order to assess the stability of a given update.&lt;/p&gt; &lt;p&gt;The idea is rather simple. The packages your team is working on and want to release are the &lt;em&gt;engineering packages&lt;/em&gt;. Given a package or a set of packages, called the &lt;em&gt;main packages&lt;/em&gt;, the Mass Prebuilder calculates the list of their direct reverse dependencies: packages that explicitly mark one of the main packages in their &lt;code&gt;BuildRequires&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;The Mass Prebuilder first builds the main packages using the distribution's facilities, which should include a set of test cases that validate general functioning. Assuming these packages are built successfully, they are then used as base packages to build the reverse dependencies and execute their own test cases.&lt;/p&gt; &lt;p&gt;This process yields a first set of results, which include successful builds (hopefully the majority), but also failures that might or might not be the result of changes introduced by modifications to the main packages. To clarify the source of the problem, as soon as a failure is detected, the Mass Prebuilder creates another mass build that includes only the reverse dependencies that failed to build during the original run. This new build runs in parallel to the original, but without the changes that were introduced into the main packages—in other words, it's a pristine build.&lt;/p&gt; &lt;p&gt;Once all the package builds are done, they can be broken down into the following categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Successfully built packages&lt;/li&gt; &lt;li&gt;Packages that failed to build only with the modified packages&lt;/li&gt; &lt;li&gt;Packages that failed to build with both the modifications and the pristine version&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The first category can likely be ignored, because it consists of packages that don't seem to have been affected by the changes.&lt;/p&gt; &lt;p&gt;The second category needs much more attention. These are the ones that cry, "Hey, there seems to be a big problem with your changes." The failures need to be analyzed to figure out the root cause of the problem—they could arise from changes that have been introduced, for instance, or maybe from a mistake made by the final user (e.g., use of a deprecated feature that got removed).&lt;/p&gt; &lt;p&gt;The last category of failures is a bit trickier. Since the build failed with the pristine packages, there may be hidden errors in them that were revealed by the new changes.&lt;/p&gt; &lt;h2&gt;How does the Mass Prebuilder work?&lt;/h2&gt; &lt;p&gt;Now let's see a bit more in detail what the Mass Prebuilder does under the hood.&lt;/p&gt; &lt;p&gt;The MPB is a set of &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; scripts that abstract the use of commonly available infrastructures. Although the infrastructure used initially is &lt;a href="https://pagure.io/copr/copr"&gt;Copr&lt;/a&gt;, there are plans to implement support for other infrastructures such as &lt;a href="https://pagure.io/koji/"&gt;Koji&lt;/a&gt; and potentially &lt;a href="https://beaker-project.org/"&gt;Beaker&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If these infrastructures are used to build the packages, the MPB is in charge of orchestrating the builds by preparing the project, calculating the list of packages to be built, and providing a simple report once everything is done.&lt;/p&gt; &lt;p&gt;Figure 1 shows a simplified overview of the system.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_0.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_0.jpg?itok=z9gvBXlt" width="600" height="276" alt="Diagram showing the Mass Prebuilder running on top of build environments and interacting with a dedicated database and configuration" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Mass Prebuilder runs on top of build environments and interacts with a dedicated database and configuration. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Under the hood, the tool has a back-end/front-end design. Most of the work is done as generically as possible, so that the back ends implement only the direct interfaces with the infrastructures.&lt;/p&gt; &lt;h2&gt;The Mass Prebuilder process&lt;/h2&gt; &lt;p&gt;The MPB set of scripts is currently available in &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;version 0.2.0&lt;/a&gt;, which allows you to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a dedicated Copr project where an engineering version of your packages can be built. This package can be provided as a source RPM (SRPM), a &lt;a href="https://github.com/release-engineering/dist-git"&gt;DistGit repository&lt;/a&gt; with a specific tag, a Git repository, or a URL providing access to an SRPM.&lt;/li&gt; &lt;li&gt;Trigger the build for these packages, and report its success or failure.&lt;/li&gt; &lt;li&gt;Automatically calculate direct reverse dependencies for the main packages, try to group them by priority based on their interdependencies, and trigger the builds for these reverse dependencies.&lt;/li&gt; &lt;li&gt;Generate a simple report on the build status for the reverse dependencies. If a reverse dependency isn't building, that triggers a new build on a pristine Copr project (without your engineering packages), and checks to see whether the packages fail there too.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As explained earlier, failures are then split into the following categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Failed: Packages that only failed in the project that included your engineering packages&lt;/li&gt; &lt;li&gt;Manual confirmation needed: Packages that failed on both sides&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At the end of the process, the tool retrieves all the available data for the packages that failed so that you can walk through them and verify whether failures are related to your package update.&lt;/p&gt; &lt;p&gt;The process is therefore decomposed into multiple stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preparation (your engineering packages are built)&lt;/li&gt; &lt;li&gt;Checking the preparation&lt;/li&gt; &lt;li&gt;Build&lt;/li&gt; &lt;li&gt;Checking the build&lt;/li&gt; &lt;li&gt;Collecting and reporting data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The process can be interrupted safely during the checks and the collecting operations. If you interrupt the Mass Prebuilder, it shows you the command to restart it later on. This checkpointing is quite useful, as some packages take a long time to build and you might not want your machine running for 30 hours straight.&lt;/p&gt; &lt;h2&gt;Configuration&lt;/h2&gt; &lt;p&gt;Prior to using the Mass Prebuilder, make sure you can communicate with the appropriate infrastructure. Prepare a small configuration file specifying what you want to build and how. Copr itself requires a &lt;code&gt;~/.config/copr&lt;/code&gt; file (as given by &lt;a href="https://copr.fedorainfracloud.org/api/"&gt;the Copr API&lt;/a&gt; when logged in) to ensure that the &lt;code&gt;copr whoami&lt;/code&gt; command gives the expected output.&lt;/p&gt; &lt;p&gt;The Mass Prebuilder configuration file can be provided in the following ways. It looks for the files in the order shown, and uses the first file it finds:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;A local &lt;code&gt;mpb.config&lt;/code&gt; file&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Through a command-line option:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mpb --config=~/work/mpb/autoconf/my_config_file&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;In the default path: &lt;code&gt;~/.mpb/config&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The following example uses a local file to show how to use the tool to check your use of the Autoconf:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir -p ~/work/mpb/autoconf $ cd ~/work/mpb/autoconf $ cat &gt; mpb1.config &lt;&lt; EOF &gt; arch: x86_64 chroot: fedora-rawhide packages: autoconf: src_type: file src: /home/fberat/work/fedora/autoconf/autoconf-2.72c-1.fc37.src.rpm data: /home/fberat/work/mpb/autoconf verbose: 1 &gt; EOF $ mpb Loading mpb.config Using copr back-end Populating package list with autoconf Executing stage 0 (prepare) Prepared build mpb.18 (ID: 18) Executing stage 1 (check_prepare) Checking build for mpb.18 (ID: 18) You can now safely interrupt this stage Restart it later using one of the following commands: "mpb --buildid 18" "mpb --buildid 18 --stage 1" Build status: / 0 out of 1 builds are done. Pending: 0 Running: 1 Success: 0 Under check: 0 Manual confirmation needed: 0 Failed: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration file can contain a lot of parameters. We'll focus here on &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;reversedeps&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;The name configuration parameter&lt;/h3&gt; &lt;p&gt;You can specify a &lt;code&gt;name&lt;/code&gt; that will help you identify your project. This name will replace &lt;code&gt;mpb.18&lt;/code&gt; from the previous example. Be careful, though, because this parameter will also be the name used in your Copr project. If the name already exists, specifying it might lead to unexpected behavior. If you choose to set the name yourself instead of having it automatically generated, I recommend adding a line to the configuration file as follows. Replace &lt;code&gt;&lt;N&gt;&lt;/code&gt; with the number given by the build, which was 18 in the previous example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ echo "build_id: &lt;N&gt;" &gt;&gt; mpb.config&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Restart the MPB later using either of the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mpb --buildid &lt;N&gt; $ mpb --buildid &lt;N&gt; --stage 1 $ mpb # If you stored the build_id in the configuration file&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The reversedeps configuration parameter&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;reversedeps&lt;/code&gt; parameter contains a list of reverse dependencies. If you specify this parameter, the Mass Prebuilder uses your list instead of calculating the reverse dependencies by checking the main packages. This parameter is useful if you want to rebuild only a subset of packages instead of, say, the 6,000+ ones for GCC.&lt;/p&gt; &lt;p&gt;An example using &lt;code&gt;reversedeps&lt;/code&gt; follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;&gt; arch: x86_64 chroot: fedora-rawhide packages: autoconf: src_type: file src: /home/fberat/work/fedora/autoconf/autoconf-2.72c-1.fc37.src.rpm reversedeps: list: libtool: priority: 0 automake: priority: 1 name: autoconf272-1 data: /home/fberat/work/mpb/autoconf verbose: 1 &gt; EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can include the &lt;code&gt;priority&lt;/code&gt; field in the reverse dependencies to specify a build order. The previous example builds &lt;code&gt;libtool&lt;/code&gt; before &lt;code&gt;automake&lt;/code&gt;. If you don't need to control the build order, the previous configuration can be simplified to:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; reversedeps: list: libtool automake&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Getting results from the Mass Prebuilder&lt;/h2&gt; &lt;p&gt;Let's come back to our Autoconf build. After a while (about 30 minutes during one test run), the tool can move to the next stage and calculate the reverse dependencies that would be valid for an x86-64 processor:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; Executing stage 2 (build) Calculating reverse dependencies. Level 0 depth for x86_64 Checking 2316 packages. Retrieved 1151 packages. Prepare discriminator for priorities calculation 100% done Setting priorities 7 pass done. Populating package list with [package names here]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, 1,151 packages are built for x86-64. The &lt;code&gt;build&lt;/code&gt; stage is followed by a &lt;code&gt;check_build&lt;/code&gt; stage:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; Executing stage 3 (check_build) Checking build for autoconf272-1 (ID: 18) You can now safely interrupt this stage Restart it later using one of the following commands: "mpb --buildid 18" "mpb --buildid 18 --stage 3" Build status: \ 3 out of 1151 builds are done. Pending: 1146 Running: 1 Success: 3 Under check: 1 Manual confirmation needed: 0 Failed: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After all these builds finish, the Mass Prebuilder collects data that you can retrieve from the following path, based on your configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;&lt;data field&gt;/&lt;name of the MPB project&gt;/&lt;build status&gt;/&lt;chroot&gt;/&lt;name of the package&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For my particular run, the paths are:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;~/work/mpb/autoconf/autoconf272-1/FAILED/fedora-rawhide-x86_64/ ~/work/mpb/autoconf/autoconf272-1/UNCONFIRMED/fedora-rawhide-x86_64/&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;A real-life example: Errors building with autoconf2.72c&lt;/h2&gt; &lt;p&gt;As of June 2022, GNU Autoconf 2.72 is still under development and is therefore unstable. Yet the upstream maintainers are tagging the mainline with intermediate versions that could be useful to test early, in order to limit the problems when the final release comes out.&lt;/p&gt; &lt;p&gt;This is where the MPB comes in handy.&lt;/p&gt; &lt;p&gt;In Fedora 36 x86_64, about 1,151 packages depend directly on Autoconf. When built with Autoconf 2.72c pre-release, we currently get the following result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Success: 1,033&lt;/li&gt; &lt;li&gt;Manual confirmation needed: 47&lt;/li&gt; &lt;li&gt;Failed: 71&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's skip the "Manual confirmation needed" packages, which failed to build with both the 2.72c version and the original 2.71 version. There may be failures to fix among them, but the "Failed" packages offer enough interesting examples for this article to keep us busy.&lt;/p&gt; &lt;p&gt;A common pattern turns up in the 71 failures: 65 of them are due to a malformed configuration script. A sample error found for the PHP package is:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;./configure: line 104153: syntax error: unexpected end of file&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although this seems to be quite a common failure, it went through the internal tests from Autoconf without being noticed.&lt;/p&gt; &lt;p&gt;Six more failures need deeper analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;am-utils: A straightforward failure due to a hard requirement for the Autoconf version (it requires 2.69 or 2.71 exclusively).&lt;/li&gt; &lt;li&gt;cyrus-imapd: A missing &lt;code&gt;krb.h&lt;/code&gt; header. Even though the configure stage reports that the file isn't available, the application still tries to use it. It's strange not to find this header, and the message may hide a bigger problem.&lt;/li&gt; &lt;li&gt;libmng: A &lt;code&gt;zlib library not found&lt;/code&gt; error because zlib-devel got installed as part of the dependencies.&lt;/li&gt; &lt;li&gt;libverto: This failure seems unrelated to Autoconf; one of its internal libraries seems to have changed its name. Yet it is strange that this failure appeared only with Autoconf 2.72c.&lt;/li&gt; &lt;li&gt;mingw-libmng: Unresolved symbols during linking. May be unrelated to Autoconf, unless a change in the configuration modified the build process.&lt;/li&gt; &lt;li&gt;nfdump: An error revealed during the configure run; the &lt;code&gt;FT2NFDUMP&lt;/code&gt; conditional was never defined. This error might also be due to a malformed configure script.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next steps, which are beyond the scope of this article, are to research the errors to gain a deeper understanding of the problems and create appropriate bug tickets for the corresponding components. Although in the current case most of the problems are likely in Autoconf itself, there may be some cases where the issues are in the user's component (such as am-utils here).&lt;/p&gt; &lt;p&gt;Overall, this sample run of the Mass Prebuilder gives a good idea of the kind of failures that would have been missed by simply building the Autoconf package and relying on its internal tests for gatekeeping. The large panel view provided by running with a distribution is quite beneficial, and can improve the overall quality of the packages being provided.&lt;/p&gt; &lt;h2&gt;Where can you find the Mass Prebuilder?&lt;/h2&gt; &lt;p&gt;As of July 2022, the tool is available as a package at &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;my Copr repository&lt;/a&gt;. Packages are available for &lt;a href="https://developers.redhat.com/articles/2022/06/07/thousands-pypi-and-rubygems-rpms-now-available-rhel-9"&gt;Extra Packages for Enterprise Linux&lt;/a&gt; (EPEL) 9 and 8, and for Fedora 35, 36, and 37.&lt;/p&gt; &lt;p&gt;The sources can be found in &lt;a href="https://gitlab.com/fberat/mass-prebuild"&gt;my GitLab repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Current limitations&lt;/h2&gt; &lt;p&gt;The Mass Prebuilder at the time of writing is still under heavy development. That means that there might be missing features you'd like to see, that there might be bugs (though I've done my best to limit them), and that interfaces may have to change a bit by the time stable releases come.&lt;/p&gt; &lt;p&gt;While I tested the tool, it appeared that builds are not fully reliable. Although reported successes are trustworthy, reported failures might not be. There were some cases where the failure cropped up because the infrastructure was incapable of installing build dependencies, even on a stable release such as Fedora 35. This failure is hard to diagnose because it doesn't make sense. The same build with no changes could result in a different status if started with a few seconds delay.&lt;/p&gt; &lt;p&gt;The data that can be collected out of a failed Copr build is relatively limited. For instance, in the problem just described, there is no way to retrieve the failing configuration script. A local build needs to be made for that.&lt;/p&gt; &lt;p&gt;If you have any suggestions, if you find an issue, or if the tool doesn't behave the way you expect, don't hesitate to contact me and give me feedback, or leave a comment at the bottom of the article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/26/find-errors-packages-through-mass-builds" title="Find errors in packages through mass builds"&gt;Find errors in packages through mass builds&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Frédéric Bérat</dc:creator><dc:date>2022-09-26T07:00:00Z</dc:date></entry><entry><title type="html">RESTEasy 6.2.0.Final Release</title><link rel="alternate" href="https://resteasy.github.io/2022/09/23/resteasy-6.2.0.Final-release/" /><author><name /></author><id>https://resteasy.github.io/2022/09/23/resteasy-6.2.0.Final-release/</id><updated>2022-09-23T18:11:11Z</updated><dc:creator /></entry><entry><title>Join the Red Hat team at NodeConf EU 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/23/join-red-hat-team-nodeconf-eu-2022" /><author><name>Lucas Holmquist</name></author><id>af2d04c4-c425-4b66-b2db-7b7fc5ba23c0</id><updated>2022-09-23T07:00:00Z</updated><published>2022-09-23T07:00:00Z</published><summary type="html">&lt;p&gt;It's that time of the year again, and NodeConf EU is almost upon us. This annual event is one of the leading &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="43652567-d1ab-4765-a588-4e905032ad7f" href="https://developers.redhat.com/topics/nodejs" title="Node.js: Develop server-side JavaScript applications"&gt;Node.js&lt;/a&gt; events in Europe. It brings together contributors and innovators from the Node.js community to deliver a wide range of talks and workshops.&lt;/p&gt; &lt;p&gt;The conference will be back in person this year after being virtual for the past two years on October 3rd–5th in Kilkenny, Ireland.&lt;/p&gt; &lt;p&gt;The Node.js team here at Red Hat will be talking about lesser-known Node.js Core modules as well as guiding attendees through a workshop that will get you familiar with cloud-native development with Node.js. &lt;/p&gt; &lt;h2&gt;Talk: Journey into mystery: Lesser-known Node Core modules and APIs&lt;/h2&gt; &lt;p&gt;Wednesday, October 4th, 2022, 9:30 UTC&lt;/p&gt; &lt;p&gt;Presenter: Luke Holmquist (&lt;a href="https://twitter.com/sienaluke"&gt;@sienaluke&lt;/a&gt;), Senior Software Engineer, Red Hat&lt;/p&gt; &lt;p&gt;One of the key concepts of Node.js is its modular architecture, and Node makes it very easy to use a wide variety of modules and &lt;a href="https://developers.redhat.com/topics/api-management"&gt;APIs&lt;/a&gt; from the community. Some of the modules and APIs that are part of Node.js Core are very familiar, like HTTP and Events. But what about those lesser-known core modules just waiting to be used? This talk will journey into mystery as we explore some of the lesser-known Core modules and APIs that Node.js offers.&lt;/p&gt; &lt;h2&gt;Workshop: Elevating Node.js applications to the cloud&lt;/h2&gt; &lt;p&gt;Wednesday, October 4th, 2022, 3:00 UTC&lt;/p&gt; &lt;p&gt;Presenters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bethany Griggs, Senior Software Engineer, Red Hat&lt;/li&gt; &lt;li&gt;Michael Dawson (&lt;a href="https://twitter.com/mhdawson1"&gt;@mhdawson1&lt;/a&gt;), Node.js Lead, Red Hat&lt;/li&gt; &lt;li&gt;Luke Holmquist (&lt;a href="https://twitter.com/sienaluke"&gt;@sienaluke&lt;/a&gt;), Senior Software Engineer, Red Hat&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This workshop provides an introduction to cloud-native development with Node.js. We will walk you through building cloud-native Node.js applications, incorporating typical components, including observability components for &lt;a href="https://developers.redhat.com/articles/2021/05/10/introduction-nodejs-reference-architecture-part-2-logging-nodejs"&gt;logging&lt;/a&gt;, metrics, and more. Next, we'll show you how to deploy your application to cloud environments. The workshop will cover cloud-native concepts and technologies, including health checks, metrics, building &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and deployments to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For a full list of the various talks and workshops, check out the &lt;a href="https://www.nodeconf.eu/agenda"&gt;NodeConf EU 2022 agenda&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Collaborator Summit&lt;/h2&gt; &lt;p&gt;There will also be a OpenJS Collaborator Summit in Dublin, Ireland on October 1-2, 2022, two days before NodeConf EU. We hope to see you there to discuss all things &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; and Node.js. Our team members will be leading or active participants in many sessions.&lt;/p&gt; &lt;p&gt;The Collab Summit is for maintainers or core contributors of an OpenJS project, plus any open source enthusiast interested in participating. This is the time for deep dives on important topics and to meet with people working across your favorite JavaScript projects. Get more details on the &lt;a href="https://openjsf.org/blog/2022/09/01/openjs-collaborator-summit-join-us-in-dublin-virtual-october-1-2%EF%BF%BC/"&gt;OpenJS website&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;More Node.js resources&lt;/h2&gt; &lt;p&gt;Don't miss the latest installments of our series on the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want to learn more about Red Hat and IBM’s involvement in the Node.js community and what we are working on, check out our topic pages at &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Red Hat Developer&lt;/a&gt; and &lt;a href="https://developer.ibm.com/languages/node-js/"&gt;IBM Developer&lt;/a&gt;. &lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/23/join-red-hat-team-nodeconf-eu-2022" title="Join the Red Hat team at NodeConf EU 2022"&gt;Join the Red Hat team at NodeConf EU 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2022-09-23T07:00:00Z</dc:date></entry><entry><title type="html">Creating your first cloud-agnostic serverless application with Java</title><link rel="alternate" href="https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html" /><author><name>Helber Belmiro</name></author><id>https://blog.kie.org/2022/09/creating-your-first-cloud-agnostic-serverless-application-with-java.html</id><updated>2022-09-22T10:35:00Z</updated><content type="html">If you are new to Serverless Workflow or serverless in general, creating a simple application for a serverless infrastructure is a good place to start. In this article, you will run through the steps to create your first serverless Java application that runs on any cloud. WHAT IS SERVERLESS? Contrary to what the name says, there are still servers in serverless, but you don’t need to worry about managing them. You just need to deploy your containers and the serverless infrastructure is responsible for providing resources to your application scale up or down. The best part is that it automatically scales up when there is a high demand or scales to zero when there is no demand. This will reduce the amount of money you spend with the cloud. WHAT WILL YOU CREATE? You will use Quarkus to create a simple Java application that returns a greeting message to an HTTP request and deploy it to Knative. WHY KNATIVE? In the beginning, serverless applications used to consist of small pieces of code that were run by a cloud vendor, like AWS Lambda. In this first phase, the applications had some limitations and were closely coupled to the vendor libraries. Knative enables developers to run serverless applications on a Kubernetes cluster. This gives you the flexibility to run your applications on any cloud, on-premises, or even mix all of them. WHY QUARKUS? Because serverless applications need to start fast. Since the biggest advantage of serverless is scale up and down (even zero) according to demand, serverless applications need to start fast when scaling up, otherwise, requests would be denied. One of the greatest characteristics of Quarkus applications is their super fast start-up. Also, Quarkus is , which means that it’s easy to deploy Quarkus applications to Kubernetes without having to understand the intricacies of the underlying Kubernetes framework. REQUIREMENTS * A local Knative installation. See . * This article uses minikube as the local Kubernetes cluster. * kn CLI installed. See . * JDK 11+ installed with JAVA_HOME configured appropriately. * Apache Maven 3.8.1+. * GraalVM (optional to deploy a native image). CREATE A QUARKUS APPLICATION &gt; NOTE: If you don’t want to create the application, you can just clone it &gt; from  and skip to  mvn io.quarkus.platform:quarkus-maven-plugin:2.11.2.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=knative-serving-quarkus-demo cd knative-serving-quarkus-demo RUN YOUR APPLICATION LOCALLY To verify that you created the project correctly, run the project locally by running the following command: mvn quarkus:dev After downloading the dependencies and building the project, you should see an output similar to: __ ____ __ _____ ___ __ ____ ______ --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,&lt; / /_/ /\ \ --\___\_\____/_/ |_/_/|_/_/|_|\____/___/ 2022-08-15 16:50:25,135 INFO [io.quarkus] (Quarkus Main Thread) knative-serving-quarkus-demo 1.0.0-SNAPSHOT on JVM (powered by Quarkus 2.11.2.Final) started in 1.339s. Listening on: http://localhost:8080 2022-08-15 16:50:25,150 INFO [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated. 2022-08-15 16:50:25,150 INFO [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, resteasy-reactive, smallrye-context-propagation, vertx] On a different terminal window or in the browser, you can access the application by sending a request to the  endpoint: curl -X 'GET' 'http://localhost:8080/hello' -H 'accept: text/plain' If you see the following output, then you have successfully created your application: Hello from RESTEasy Reactive Hit Ctrl + C to stop the application. PREPARE YOUR APPLICATION FOR DEPLOYMENT TO KNATIVE ADD THE REQUIRED DEPENDENCIES Add the following dependencies to the pom.xml file: &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-kubernetes&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-container-image-jib&lt;/artifactId&gt; &lt;/dependency&gt; CONFIGURE THE APPLICATION FOR DEPLOYMENT TO KNATIVE Add the following configuration to the src/main/resources/application.properties file: quarkus.kubernetes.deployment-target=knative quarkus.container-image.group=dev.local/hbelmiro &gt; NOTE: In the quarkus.container-image.group property, replace hbelmiro with &gt; your container registry username. DEPLOY YOUR APPLICATION TO KNATIVE START THE MINIKUBE TUNNEL &gt; NOTE: This step is only necessary if you are using minikube as the local &gt; Kubernetes cluster. On a different terminal window, run the following command to start the minikube tunnel: minikube tunnel --profile knative You should see an output similar to the following: Status: machine: knative pid: 223762 route: 10.96.0.0/12 -&gt; 192.168.49.2 minikube: Running services: [kourier] errors: minikube: no errors router: no errors loadbalancer emulator: no errors Leave the terminal window open and running the above command. CONFIGURE THE CONTAINER CLI TO USE THE CONTAINER ENGINE INSIDE MINIKUBE eval $(minikube -p knative docker-env) DEPLOY THE APPLICATION Run the following command to deploy the application to Knative: mvn clean package -Dquarkus.kubernetes.deploy=true You should see an output similar to the following: [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Deploying to knative server: https://192.168.49.2:8443/ in namespace: default. [INFO] [io.quarkus.kubernetes.deployment.KubernetesDeployer] Applied: Service knative-serving-quarkus-demo. [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 8952ms [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ CHECK THE KNATIVE DEPLOYED SERVICES Run the following command to check the Knative deployed services: kn service list You should see your application listed on the deployed services like the following: NAME URL LATEST AGE CONDITIONS READY REASON knative-serving-quarkus-demo http://knative-serving-quarkus-demo.default.10.106.207.219.sslip.io knative-serving-quarkus-demo-00001 23s 3 OK / 3 True &gt; IMPORTANT: In the above output, check the READY status of the application. If &gt; the status is not True, then you need to wait for the application to be ready, &gt; or there is a problem with the deployment. SEND A REQUEST TO THE DEPLOYED APPLICATION Use the URL returned by the above command to send a request to the deployed application. curl -X 'GET' 'http://knative-serving-quarkus-demo.default.10.106.207.219.sslip.io/hello' -H 'accept: text/plain' You should see the following output: Hello from RESTEasy Reactive GOING NATIVE You can create a native image of your application to make it start even faster. To do that, deploy your application by using the following command: mvn clean package -Pnative -Dquarkus.native.native-image-xmx=4096m -Dquarkus.native.remote-container-build=true -Dquarkus.kubernetes.deploy=true &gt; IMPORTANT: -Dquarkus.native.native-image-xmx=4096m is the amount of memory &gt; Quarkus can use to generate the native image. You should adjust it or &gt; completely remove it depending on your local machine’s specifications. NOW YOU ARE READY TO RUN SERVERLESS APPLICATIONS USING JAVA Easy, isn’t it? Quarkus and Knative give you the freedom to run serverless applications using Java on-premises or in the cloud, no matter the vendor. You can even mix more than one cloud vendor with your on-premises infrastructure. This flexibility brings you agility and reduces your costs with infrastructure. NEXT STEP If you want to go further on serverless with more exciting stuff, check out  The post appeared first on .</content><dc:creator>Helber Belmiro</dc:creator></entry></feed>
